{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8798, 0.7266, 0.4198],\n",
      "        [0.7841, 0.9321, 0.8293],\n",
      "        [0.2777, 0.4361, 0.8149]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(10), nn.ReLU(), nn.LazyLinear(1))\n",
    "X = torch.rand(size=(3,3))\n",
    "print(X)\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.1875, -0.2367, -0.2676,  0.2621, -0.1305, -0.1872,  0.0413,  0.0446,\n",
       "                       -0.3151,  0.3016]])),\n",
       "             ('bias', tensor([0.2822]))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can access each layer by indexing it as follows and ge taccess to all its parameters\n",
    "# from 0th to the n-1 (last) layer for sequential, Activations also count as individual layers\n",
    "net[4].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.parameter.Parameter,\n",
       " Parameter containing:\n",
       " tensor([[ 0.0984, -0.0923, -0.1363,  0.1738,  0.2956, -0.0867,  0.0862,  0.2050],\n",
       "         [-0.1523,  0.3298, -0.3459,  0.3285,  0.0314,  0.0276,  0.1756, -0.0098],\n",
       "         [ 0.1732, -0.2964, -0.1219, -0.0433, -0.0764, -0.3146,  0.0256,  0.0288],\n",
       "         [-0.3312,  0.1796,  0.0249,  0.1019, -0.2635, -0.2969, -0.2857, -0.0430],\n",
       "         [ 0.1927,  0.1466,  0.1671,  0.3324,  0.0239,  0.0535, -0.2943, -0.0885],\n",
       "         [-0.3306,  0.3133, -0.2776,  0.3406, -0.1077, -0.0114,  0.2980,  0.0090],\n",
       "         [-0.1945,  0.2282,  0.2173,  0.3081, -0.0949,  0.3322,  0.1798,  0.1888],\n",
       "         [-0.2451, -0.0597,  0.2713, -0.2248,  0.2964, -0.1583,  0.0893, -0.2207],\n",
       "         [ 0.2466, -0.0163, -0.0373, -0.0967, -0.3260,  0.2582, -0.2347,  0.1938],\n",
       "         [ 0.2043, -0.0547,  0.2155, -0.0607, -0.2758,  0.2062,  0.0637,  0.2381]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get direct access to the value of parameters: bias or weight\n",
    "# type(net[2].bias), net[2].bias.data\n",
    "type(net[2].weight), net[2].weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight', torch.Size([8, 3])),\n",
       " ('0.bias', torch.Size([8])),\n",
       " ('2.weight', torch.Size([10, 8])),\n",
       " ('2.bias', torch.Size([10])),\n",
       " ('4.weight', torch.Size([1, 10])),\n",
       " ('4.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View all parameters at once\n",
    "[(name, param.shape) for name, param in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# We can make models share the same layer across space and time as a referenced\n",
    "\n",
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "net(X)\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data == net[4].weight.data)\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
