{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File I/O\n",
    "- This section will help me understand how to load and save results or data from Disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(4)\n",
    "# Save a tensor\n",
    "torch.save(x, 'x-file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tensor from file `x-file`\n",
    "x2 = torch.load(\"x-file\")\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can save a dictionary or even lists\n",
    "mydict = {\"x\": x, \"y\": y}\n",
    "torch.save(mydict, \"mydict\")\n",
    "mydict2 = torch.load(\"mydict\")\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sih/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# Finally we can save a model at a particular state to disk and load it later\n",
    "# This is done by storing the parameters\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.output = nn.LazyLinear(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.0406, -0.0169,  0.1534,  ..., -0.1299, -0.2089,  0.1654],\n",
       "                      [-0.1344, -0.2185, -0.0614,  ..., -0.0782,  0.2068,  0.0558],\n",
       "                      [-0.1868,  0.1597,  0.0736,  ..., -0.2019,  0.2121, -0.2121],\n",
       "                      ...,\n",
       "                      [-0.0317,  0.0709, -0.0387,  ...,  0.2012,  0.1614, -0.1359],\n",
       "                      [-0.0844,  0.1391, -0.2071,  ...,  0.1102,  0.1514,  0.0312],\n",
       "                      [-0.1129,  0.2140, -0.1225,  ..., -0.0887, -0.0540,  0.0505]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([ 0.0074, -0.0040, -0.0580,  0.0221, -0.0811,  0.0756,  0.0103, -0.1469,\n",
       "                       0.1645, -0.1613, -0.1643, -0.2178, -0.1158, -0.1434, -0.1250, -0.0504,\n",
       "                      -0.1854,  0.0131,  0.1311,  0.1596, -0.1383, -0.1438,  0.0212, -0.2230,\n",
       "                      -0.2089,  0.1713, -0.1243, -0.1652,  0.1485,  0.0142,  0.2021, -0.2169,\n",
       "                      -0.0281, -0.2058, -0.1107, -0.0887, -0.0842, -0.1480,  0.1441, -0.1298,\n",
       "                      -0.0634, -0.1301,  0.2047, -0.2179,  0.0280, -0.0451,  0.1927,  0.1773,\n",
       "                      -0.1794,  0.0043, -0.0629,  0.2084,  0.1185,  0.1954, -0.2125,  0.0804,\n",
       "                      -0.2090, -0.1327, -0.1089,  0.0750,  0.1316, -0.1991, -0.1013,  0.0515,\n",
       "                       0.1974, -0.0644, -0.1930, -0.0591, -0.0110, -0.1632, -0.1787,  0.1536,\n",
       "                       0.1217,  0.1799,  0.0462, -0.2040, -0.1625, -0.1973, -0.0204, -0.1095,\n",
       "                       0.0676,  0.2067, -0.1031,  0.1985, -0.1560,  0.0808, -0.0407,  0.0204,\n",
       "                       0.0764,  0.1899,  0.0267, -0.0446,  0.2053,  0.2232, -0.1732, -0.1926,\n",
       "                       0.0707,  0.1781, -0.1056, -0.0494,  0.1658,  0.0688,  0.0232, -0.0072,\n",
       "                       0.0399, -0.1215, -0.1178,  0.1843, -0.1010, -0.1509,  0.0287,  0.0747,\n",
       "                       0.1300, -0.1410,  0.0322, -0.2111, -0.0727, -0.1401, -0.1885,  0.0170,\n",
       "                       0.1009, -0.2129, -0.0204,  0.0602,  0.2127,  0.0573,  0.0701, -0.0752,\n",
       "                      -0.1281, -0.0139,  0.1713, -0.0293, -0.1503,  0.0218,  0.1859,  0.1276,\n",
       "                       0.2054, -0.1389, -0.1992, -0.1178,  0.1628, -0.1832, -0.1236,  0.2086,\n",
       "                      -0.1951, -0.1903, -0.1765, -0.0529, -0.0984, -0.0910,  0.2193,  0.0843,\n",
       "                      -0.1850, -0.1450, -0.0156, -0.1255,  0.0890,  0.1602, -0.1321, -0.0316,\n",
       "                       0.1665, -0.1865, -0.0222, -0.1351, -0.1084,  0.0646,  0.1650, -0.0249,\n",
       "                       0.0034, -0.1127, -0.0034,  0.0539,  0.2067,  0.1204, -0.0420,  0.2052,\n",
       "                       0.2163,  0.0706, -0.0492, -0.0841,  0.0185, -0.2074,  0.2210,  0.0781,\n",
       "                       0.0995,  0.0398,  0.1125, -0.0506, -0.0099,  0.1813, -0.1295,  0.1623,\n",
       "                      -0.1655,  0.1763, -0.1931, -0.1983,  0.0796,  0.1947,  0.1622, -0.1856,\n",
       "                      -0.1886, -0.0973,  0.1048, -0.0576,  0.0139,  0.1042,  0.2216,  0.1373,\n",
       "                      -0.0107,  0.1405, -0.1048, -0.2179, -0.0409,  0.1809, -0.1540,  0.1860,\n",
       "                      -0.2104, -0.2182, -0.1856,  0.0424, -0.0571, -0.1030, -0.0897,  0.1555,\n",
       "                      -0.0102,  0.1683,  0.1859, -0.1762, -0.1837,  0.2123,  0.1000, -0.1984,\n",
       "                       0.1406,  0.1036, -0.0440,  0.1833,  0.1093,  0.0766,  0.1687,  0.0860,\n",
       "                      -0.0879, -0.1337,  0.1189,  0.1780, -0.1510,  0.0184,  0.0755, -0.0367,\n",
       "                      -0.0863,  0.1019, -0.1501,  0.0037,  0.2062, -0.1282,  0.1681,  0.1933])),\n",
       "             ('output.weight',\n",
       "              tensor([[ 0.0562, -0.0043,  0.0451,  ..., -0.0130, -0.0597,  0.0355],\n",
       "                      [ 0.0578,  0.0265, -0.0357,  ..., -0.0414, -0.0218,  0.0603],\n",
       "                      [ 0.0505, -0.0022,  0.0209,  ...,  0.0173, -0.0139,  0.0328],\n",
       "                      ...,\n",
       "                      [ 0.0443, -0.0528,  0.0437,  ..., -0.0266, -0.0081,  0.0520],\n",
       "                      [-0.0025,  0.0461,  0.0275,  ..., -0.0440,  0.0468,  0.0539],\n",
       "                      [-0.0052, -0.0499, -0.0282,  ..., -0.0303, -0.0035, -0.0268]])),\n",
       "             ('output.bias',\n",
       "              tensor([-0.0439,  0.0262,  0.0496,  0.0022, -0.0464, -0.0508,  0.0597, -0.0592,\n",
       "                       0.0573, -0.0228]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('mlp.parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sih/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "  (output): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most imporatntly we can recover a new model at that state\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.parameters'))\n",
    "clone.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
