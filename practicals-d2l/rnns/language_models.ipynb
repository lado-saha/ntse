{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.TimeMachine)  # @save\n",
    "def __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):\n",
    "    \"\"\"\n",
    "    num_steps: The number of token in a single time-step\n",
    "    \"\"\"\n",
    "    super(d2l.TimeMachine, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    # Download the Timemachine.txt, tokenize it, then vocabularizes it\n",
    "    corpus, self.vocab = self.build(self._download())\n",
    "\n",
    "    # This array contains the list of tokens partitioned into num_step-lengthed tokens. This will permit us to later sample mini-batches\n",
    "    array = torch.tensor(\n",
    "        [corpus[i : i + num_steps + 1] for i in range(len(corpus) - num_steps)]\n",
    "    )\n",
    "    # Notice that Y is always 1-timestep(corresponding to 1 token) ahead of of X.\n",
    "    self.X, self.Y = array[:, :-1], array[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.TimeMachine)  # @save\n",
    "def get_dataloader(self, train):\n",
    "    # This is the list of token indices which will be used to load the dataset in case of training or validation\n",
    "    # Training_set_size = num_train and the rest will be used for validation until we reach the validation_set_size = num_val\n",
    "    idx = (\n",
    "        slice(0, self.num_train)\n",
    "        if train\n",
    "        else slice(self.num_train, self.num_train + self.num_val)\n",
    "    )\n",
    "    return self.get_tensorloader([self.X, self.Y], train, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[14,  6, 15, 20, 10, 16, 15, 20,  0,  7],\n",
      "        [ 0, 20,  4,  2, 21, 21,  6, 19,  6,  5]]) \n",
      "Y: tensor([[ 6, 15, 20, 10, 16, 15, 20,  0,  7, 16],\n",
      "        [20,  4,  2, 21, 21,  6, 19,  6,  5,  0]])\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset for verification\n",
    "data = d2l.TimeMachine(batch_size=2, num_steps=10)\n",
    "for X, Y in data.train_dataloader():\n",
    "    print(\"X:\", X, \"\\nY:\", Y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To train a language model, we randomly sample pairs of of input sequence and target sequences in minibatches, after training, we use the **perplexity** to measure the language model quality\n",
    "\n",
    "- **Perplexity**: Is the exponentiation of the sum of the information (-log P) carried by succesively getting x_t token using the x_t-1 until 1 token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
